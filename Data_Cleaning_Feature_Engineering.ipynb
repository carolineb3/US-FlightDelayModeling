{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineb3/US-FlightDelayModeling/blob/main/Data_Cleaning_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhTIBzgyXuxY"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieve and Combine Datasets, Remove Cancelled/Diverted Flights**"
      ],
      "metadata": {
        "id": "JmAwwg-LMB4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZCkp8UlebjP",
        "outputId": "e67b3901-4b64-42a2-dc26-81af1a104561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Shape of dataset: (1809857, 27)\n",
            "Columns: Index(['YEAR', 'MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER',\n",
            "       'ORIGIN_AIRPORT_ID', 'ORIGIN', 'DEST_AIRPORT_ID', 'DEST',\n",
            "       'CRS_DEP_TIME', 'DEP_DELAY_NEW', 'DEP_TIME_BLK', 'CRS_ARR_TIME',\n",
            "       'ARR_TIME', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_TIME_BLK', 'CANCELLED',\n",
            "       'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'DISTANCE',\n",
            "       'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY',\n",
            "       'LATE_AIRCRAFT_DELAY'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load and merge data\n",
        "path = '/content/drive/MyDrive/MIS 545 Project/Raw Data Files'\n",
        "\n",
        "may = pd.read_csv(path + '/May_Reports.csv')\n",
        "june = pd.read_csv(path + '/June_Reports.csv')\n",
        "july = pd.read_csv(path + '/July_Reports.csv')\n",
        "\n",
        "df_raw = pd.concat([may, june, july], ignore_index=True)\n",
        "\n",
        "# Remove cancelled and diverted flights out from dataset\n",
        "df_clean = df_raw[(df_raw['CANCELLED'] == 0) & (df_raw['DIVERTED'] == 0)].copy()\n",
        "\n",
        "print(f'Shape of dataset: {df_clean.shape}')\n",
        "print(f'Columns: {df_clean.columns}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Leakage Columns and Feature Engineering**"
      ],
      "metadata": {
        "id": "35n1LsBFMP8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove columns to prevent leakage\n",
        "df_clean = df_clean.drop(columns=[\n",
        "    \"DEP_DELAY_NEW\",\"ARR_TIME\",\n",
        "    \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\n",
        "    \"LATE_AIRCRAFT_DELAY\",\"ACTUAL_ELAPSED_TIME\"], errors='ignore')\n",
        "\n",
        "# Feature engineering\n",
        "\n",
        "# Convert FL_DATE to datetime\n",
        "df_clean['FL_DATE'] = pd.to_datetime(df_clean['FL_DATE'],format='%m/%d/%Y %I:%M:%S %p',errors='coerce')\n",
        "df_clean['MONTH'] = df_clean['FL_DATE'].dt.month\n",
        "df_clean['DAY_OF_WEEK'] = df_clean['DAY_OF_WEEK'].astype(int)\n",
        "\n",
        "# Convert CRS_DEP_TIME into HOUR\n",
        "df_clean['CRS_DEP_TIME'] = pd.to_numeric(df_clean['CRS_DEP_TIME'], errors='coerce').fillna(0).astype(int)\n",
        "df_clean['HOUR'] = df_clean['CRS_DEP_TIME'].astype(str).str.zfill(4).str[:2].astype(int)\n",
        "\n",
        "# Categorical\n",
        "df_clean['OP_UNIQUE_CARRIER'] = df_clean['OP_UNIQUE_CARRIER'].astype('category')\n",
        "df_clean['ORIGIN'] = df_clean['ORIGIN'].astype('category')\n",
        "df_clean['DEST'] = df_clean['DEST'].astype('category')\n",
        "df_clean['DEP_TIME_BLK'] = df_clean['DEP_TIME_BLK'].astype('category')\n",
        "\n",
        "# Frequency Encoding for ORIGIN & DEST\n",
        "\n",
        "# Compute frequency of each airport in the full dataset\n",
        "origin_freq = df_clean['ORIGIN'].value_counts()\n",
        "dest_freq = df_clean['DEST'].value_counts()\n",
        "\n",
        "# Add frequency encoded columns to df_clean\n",
        "df_clean['ORIGIN_FREQ'] = df_clean['ORIGIN'].map(origin_freq)\n",
        "df_clean['DEST_FREQ'] = df_clean['DEST'].map(dest_freq)\n",
        "\n",
        "# Drop original high-cardinality columns of ORIGIN/DEST\n",
        "df_clean = df_clean.drop(columns=['ORIGIN', 'DEST'], errors='ignore')\n",
        "\n",
        "# Drop FL_DATE after extracting Month and Day\n",
        "df_clean = df_clean.drop(columns=['FL_DATE'], errors='ignore')\n",
        "\n",
        "print('Created Features:')\n",
        "print(f'MONTH: {sorted(df_clean['MONTH'].unique())}')\n",
        "print(f'HOUR: range{df_clean['HOUR'].min()}-{df_clean['HOUR'].max()}')\n",
        "print(f'DAY_OF_WEEK: {df_clean['DAY_OF_WEEK'].unique()}')\n",
        "print(f'OP_UNIQUE_CARRIER: {df_clean[\"OP_UNIQUE_CARRIER\"].unique()}')\n",
        "print(f'DEP_TIME_BLK: {df_clean[\"DEP_TIME_BLK\"].unique()}')\n",
        "print(f'ORIGIN_FREQ: {df_clean[\"ORIGIN_FREQ\"].min()}-{df_clean[\"ORIGIN_FREQ\"].max()}')\n",
        "print(f'DEST_FREQ: {df_clean[\"DEST_FREQ\"].min()}-{df_clean[\"DEST_FREQ\"].max()}')\n",
        "\n",
        "# Check for any missing values\n",
        "missing = df_clean.isnull().sum()\n",
        "missing = missing[missing >0]\n",
        "\n",
        "if len(missing) > 0:\n",
        "  print('Missing values found')\n",
        "else:\n",
        "  print('No missing values found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6wVB9-lHa3I",
        "outputId": "104544da-8bb0-450b-a3a0-f2a807f8a03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Features:\n",
            "MONTH: [np.int32(5), np.int32(6), np.int32(7)]\n",
            "HOUR: range0-24\n",
            "DAY_OF_WEEK: [1 2 3 4 5 6 7]\n",
            "OP_UNIQUE_CARRIER: ['AA', 'AS', 'B6', 'DL', 'F9', ..., 'OH', 'OO', 'UA', 'WN', 'YX']\n",
            "Length: 14\n",
            "Categories (14, object): ['AA', 'AS', 'B6', 'DL', ..., 'OO', 'UA', 'WN', 'YX']\n",
            "DEP_TIME_BLK: ['0001-0559', '0600-0659', '0700-0759', '1000-1059', '1300-1359', ..., '1800-1859', '0800-0859', '1900-1959', '2200-2259', '2300-2359']\n",
            "Length: 19\n",
            "Categories (19, object): ['0001-0559', '0600-0659', '0700-0759', '0800-0859', ..., '2000-2059',\n",
            "                          '2100-2159', '2200-2259', '2300-2359']\n",
            "ORIGIN_FREQ: 25-86562\n",
            "DEST_FREQ: 25-86462\n",
            "No missing values found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split for Regression Dataset**"
      ],
      "metadata": {
        "id": "dSztym-CcNcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_reg = df_clean['ARR_DELAY_NEW']\n",
        "X_reg = df_clean.drop(columns=['ARR_DELAY_NEW','ARR_DEL15'], errors='ignore')\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg,\n",
        "    test_size=0.2,\n",
        "    random_state=37\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train_reg.shape)\n",
        "print(\"Test size:\", X_test_reg.shape)\n",
        "\n",
        "print(\"Regression split complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXkZlRCScMiB",
        "outputId": "48973f37-c303-4795-c01f-3d7d952e937c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (1447885, 17)\n",
            "Test size: (361972, 17)\n",
            "Regression split complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split for Classification Dataset**"
      ],
      "metadata": {
        "id": "SolC194rMOth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create classification X and y\n",
        "X_clf_full = df_clean.drop(columns=['ARR_DEL15'])\n",
        "y_clf_full = df_clean['ARR_DEL15']\n",
        "\n",
        "# Split classification X and y into train and test\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X_clf_full, y_clf_full,\n",
        "    test_size=0.2,\n",
        "    stratify=y_clf_full,\n",
        "    random_state=37\n",
        ")\n",
        "\n",
        "# Combine train set for sampling\n",
        "train_df = X_train_clf.copy()\n",
        "train_df['ARR_DEL15'] = y_train_clf.values\n",
        "\n",
        "delayed = train_df[train_df['ARR_DEL15'] == 1]\n",
        "ontime = train_df[train_df['ARR_DEL15'] == 0]\n",
        "\n",
        "# Downsample majority (on time)\n",
        "ontime_down = ontime.sample(n=len(delayed), random_state=37)\n",
        "\n",
        "train_balanced = pd.concat([delayed, ontime_down], ignore_index=True)\n",
        "train_balanced = train_balanced.sample(frac=1, random_state=37).reset_index(drop=True)\n",
        "\n",
        "# Split train sets again\n",
        "X_train_clf = train_balanced.drop(columns=['ARR_DEL15'])\n",
        "y_train_clf = train_balanced['ARR_DEL15']\n",
        "\n",
        "print(\"Balanced train distribution:\")\n",
        "print(y_train_clf.value_counts(normalize=True))\n",
        "\n",
        "print(\"Unchanged test distribution:\")\n",
        "print(y_test_clf.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "8AYKE-gikTCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8048ad-2f60-4504-97db-b0adbdf9f021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced train distribution:\n",
            "ARR_DEL15\n",
            "1.0    0.5\n",
            "0.0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Unchanged test distribution:\n",
            "ARR_DEL15\n",
            "0.0    0.730667\n",
            "1.0    0.269333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save separate train/test splits for classification and regression models**"
      ],
      "metadata": {
        "id": "yL_ttThgUHlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/MyDrive/MIS 545 Project/'\n",
        "\n",
        "# Save classification splits\n",
        "X_train_clf.to_csv(save_dir + 'X_train_clf.csv', index=False)\n",
        "X_test_clf.to_csv(save_dir + 'X_test_clf.csv', index=False)\n",
        "y_train_clf.to_csv(save_dir + 'y_train_clf.csv', index=False)\n",
        "y_test_clf.to_csv(save_dir + 'y_test_clf.csv', index=False)"
      ],
      "metadata": {
        "id": "TMZd7vEmUHIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save regression splits\n",
        "\n",
        "X_train_reg.to_csv(save_dir + 'X_train_reg.csv', index=False)\n",
        "X_test_reg.to_csv(save_dir + 'X_test_reg.csv', index=False)\n",
        "y_train_reg.to_csv(save_dir + 'y_train_reg.csv', index=False)\n",
        "y_test_reg.to_csv(save_dir + 'y_test_reg.csv', index=False)"
      ],
      "metadata": {
        "id": "9aSLyrNojo72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saved preprocessor for regression:**"
      ],
      "metadata": {
        "id": "pvv577jjPHM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [\n",
        "    'CRS_DEP_TIME',\n",
        "    'CRS_ELAPSED_TIME',\n",
        "    'DISTANCE',\n",
        "    'MONTH',\n",
        "    'DAY_OF_WEEK',\n",
        "    'HOUR',\n",
        "    'ORIGIN_FREQ',\n",
        "    'DEST_FREQ\n",
        "]\n",
        "categorical_features = [\n",
        "    'OP_UNIQUE_CARRIER',\n",
        "    'DEP_TIME_BLK'\n",
        "]\n",
        "\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_pipeline, numeric_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "save_path = '/content/drive/MyDrive/MIS 545 Project/preprocessor_final.joblib'\n",
        "joblib.dump(preprocessor, save_path)\n",
        "\n",
        "print(f\"Preprocessor saved to: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "928sFDHCPScP",
        "outputId": "43a15bb4-c522-4048-e5a6-74155ed8b2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessor saved to: /content/drive/MyDrive/MIS 545 Project/preprocessor_final.joblib\n"
          ]
        }
      ]
    }
  ]
}